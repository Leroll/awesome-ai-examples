{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 01 20:42:55 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 19%   51C    P5    28W / 260W |    715MiB / 11264MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1584    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      2960    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6192    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     10412    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10560    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     11724    C+G   ...86)\\滴答清单\\TickTick.exe    N/A      |\n",
      "|    0   N/A  N/A     12764    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     13216    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13236    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     15208    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     15700    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     17612    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     17872    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     19664    C+G   ...砑�\\同花顺\\hxexternal.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1623159150372,
     "user": {
      "displayName": "ja sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_Lc-2kDJwwgkAnne2v1zxyuxlU9A73axWpORH=s64",
      "userId": "13107892294865815341"
     },
     "user_tz": -480
    },
    "id": "QgyPpE5By2iP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import dataloader\n",
    "from time import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cost(func):\n",
    "    def Wrapper(*arg, **kargs):\n",
    "        t0 = time()\n",
    "        res = func(*arg, **kargs)\n",
    "        t1 = time()\n",
    "        print(f'[{func.__name__}] cost {t1-t0:.2f}s')\n",
    "        \n",
    "        return res\n",
    "    return Wrapper        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityDataProcessor:\n",
    "    \"\"\"\n",
    "    处理如下类型数据集\n",
    "    [q1 q2 label]\n",
    "\n",
    "    [idx q1 q2 label]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger=print):\n",
    "        self.logger = logger\n",
    "\n",
    "    def process(self, name, path,\n",
    "                sep='\\t',\n",
    "                has_index=False,\n",
    "                batch_size=32,\n",
    "                is_shuffle=True):\n",
    "\n",
    "        data = self.read_data(name, path, sep, has_index)\n",
    "        dataloader = self.create_dataloader(data, batch_size, is_shuffle)\n",
    "\n",
    "        return data, dataloader\n",
    "\n",
    "    def read_data(self,mode, name, path, sep, encoder='utf-8', has_index=False):\n",
    "        \"\"\"\n",
    "        读取数据,返回 list形式的数据\n",
    "        \n",
    "        mode: 读取数据的方式, \n",
    "              readline \n",
    "              pandas \n",
    "        \"\"\"\n",
    "        self.logger(f'-'*42)\n",
    "        self.logger(f'start to read: [{name}]...')\n",
    "        \n",
    "        if mode == 'readline':\n",
    "            data = self._read_data_by_readline(path=path, \n",
    "                                         sep=sep, \n",
    "                                         encoder=encoder,\n",
    "                                         has_index=has_index)\n",
    "        elif mode == 'pandas':\n",
    "            data = self._read_data_by_pandas(path=path, \n",
    "                                       sep=sep, \n",
    "                                       encoder=encoder)\n",
    "        else:\n",
    "            raise Exception('mode的值有误')\n",
    "        \n",
    "        self.logger(f'finish reading: [{name}]')\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def _read_data_by_readline(self, path, sep, encoder='utf-8', has_index=False):\n",
    "\n",
    "        data = []\n",
    "        with open(path, encoding=encoder) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                try:\n",
    "                    # 预处理\n",
    "                    line = line.strip()\n",
    "                    line = line.replace('\\ufeff', '')\n",
    "\n",
    "                    if has_index:\n",
    "                        idx, q1, q2, label = line.split(sep)\n",
    "                    else:\n",
    "                        q1, q2, label = line.split(sep)\n",
    "                    data.append([q1, q2, label])\n",
    "\n",
    "                    line = f.readline()\n",
    "                except Exception as e:\n",
    "                    print(f'line: {line}')\n",
    "                    print('-'*42)\n",
    "                    print(e)\n",
    "                    sys.exit()\n",
    "                    \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def _read_data_by_pandas(self,path, sep, encoder='utf-8'):\n",
    "        \n",
    "        data = pd.read_csv(path, sep=sep, encoding=encoder)\n",
    "        data = data.to_numpy().tolist()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "\n",
    "    def create_dataloader(self, data, batch_size, is_shuffle):\n",
    "        dataloader = torch.utils.data.DataLoader(data,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=is_shuffle)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "data_processor = SimilarityDataProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bq_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "start to read: [bq_corpus_train]...\n",
      "finish reading: [bq_corpus_train]\n",
      "------------------------------------------\n",
      "start to read: [bq_corpus_val]...\n",
      "finish reading: [bq_corpus_val]\n",
      "------------------------------------------\n",
      "start to read: [bq_corpus_test]...\n",
      "finish reading: [bq_corpus_test]\n"
     ]
    }
   ],
   "source": [
    "path = './data/bq_corpus/'\n",
    "bq_corpus_train = data_processor.read_data(mode='pandas',\n",
    "                                           name='bq_corpus_train', \n",
    "                                           path=path+'train.csv', \n",
    "                                           sep=',',\n",
    "                                           encoder='utf-8')\n",
    "\n",
    "bq_corpus_val = data_processor.read_data(mode='pandas',\n",
    "                                         name='bq_corpus_val', \n",
    "                                         path=path+'dev.csv', \n",
    "                                         sep=',',\n",
    "                                         encoder='utf-8')\n",
    "\n",
    "bq_corpus_test = data_processor.read_data(mode='pandas',\n",
    "                                          name='bq_corpus_test', \n",
    "                                          path=path+'test.csv', \n",
    "                                          sep=',',\n",
    "                                          encoder='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train,val,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exchange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['用微信都6年，微信没有微粒贷功能', '4。  号码来微粒贷', 0],\n",
       " ['微信消费算吗', '还有多少钱没还', 0],\n",
       " ['交易密码忘记了找回密码绑定的手机卡也掉了', '怎么最近安全老是要改密码呢好麻烦', 0],\n",
       " ['你好 我昨天晚上申请的没有打电话给我 今天之内一定会打吗？', '什么时候可以到账', 0],\n",
       " ['“微粒贷开通\"', '你好，我的微粒贷怎么没有开通呢', 0]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = bq_corpus_train\n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4。  号码来微粒贷', '用微信都6年，微信没有微粒贷功能', 0),\n",
       " ('还有多少钱没还', '微信消费算吗', 0),\n",
       " ('怎么最近安全老是要改密码呢好麻烦', '交易密码忘记了找回密码绑定的手机卡也掉了', 0),\n",
       " ('什么时候可以到账', '你好 我昨天晚上申请的没有打电话给我 今天之内一定会打吗？', 0),\n",
       " ('你好，我的微粒贷怎么没有开通呢', '“微粒贷开通\"', 0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_change = list(zip(*train))\n",
    "train_change = [train_change[1],train_change[0],train_change[2]]\n",
    "train_change = list(zip(*train_change))\n",
    "train_change[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train+train_change\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "val, test\n",
    "\"\"\"\n",
    "val = bq_corpus_val\n",
    "test = bq_corpus_test\n",
    "\n",
    "len(val), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_processor.create_dataloader(train, \n",
    "                                                batch_size=batch_size, \n",
    "                                                is_shuffle=True)\n",
    "\n",
    "val_loader = data_processor.create_dataloader(val, \n",
    "                                              batch_size=batch_size, \n",
    "                                              is_shuffle=False)\n",
    "\n",
    "test_loader = data_processor.create_dataloader(test, \n",
    "                                               batch_size=batch_size, \n",
    "                                               is_shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrain models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "path = '../a_nlp_resource/transformers/bert-base-chinese/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "bert = BertModel.from_pretrained(path).to(device)\n",
    "bert.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer([\"我的太阳\", '我'], \n",
    "           padding='max_length', \n",
    "           truncation=True, \n",
    "           max_length=20,\n",
    "           return_tensors = 'pt'\n",
    "           ).to(device)\n",
    "y = bert(**x)\n",
    "y[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetenceBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        global bert, tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert = bert\n",
    "        \n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.linear = nn.Linear(hidden_size*3, 2)\n",
    "        \n",
    "    def _BertModel_version_1(self, q):\n",
    "        # 全体求mean，初版\n",
    "        q = self.tokenizer(q, \n",
    "                           padding='max_length', \n",
    "                           truncation=True, \n",
    "                           max_length=max_len,\n",
    "                           return_tensors = 'pt'\n",
    "                           ).to(device)\n",
    "        q = self.bert(**q)[0]\n",
    "        \n",
    "        q = torch.mean(q, dim=1)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def _BertModel_version_2(self, q):\n",
    "        # hidden output 去掉 padding 的影响\n",
    "        token = self.tokenizer(q, \n",
    "                           padding='max_length', \n",
    "                           truncation=True, \n",
    "                           max_length=max_len,\n",
    "                           return_tensors = 'pt'\n",
    "                           ).to(device)\n",
    "        q = self.bert(**token)[0] # hidden \n",
    "        \n",
    "        attention_mask = torch.unsqueeze(token['attention_mask'], 2)\n",
    "        q = (attention_mask*q).sum(1)/attention_mask.sum(1)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    \n",
    "    def forward(self,q1,q2):\n",
    "        q1 = self._BertModel_version_1(q1)\n",
    "        q2 = self._BertModel_version_1(q2)\n",
    "           \n",
    "        diff = torch.abs(q1-q2)\n",
    "        h = torch.cat((q1,q2,diff),dim=-1)\n",
    "        h = self.linear(h)\n",
    "        \n",
    "        return h   \n",
    "    \n",
    "    def save(self, path='./sbert.model'):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load(self, path='./sbert.model'):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBert = SetenceBert().to(device)\n",
    "sBert.save('init_sbert.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TrainProcessor():\n",
    "    def __init__(self, model, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        \n",
    "        self.train = train_loader\n",
    "        self.val = val_loader\n",
    "        \n",
    "        self._set_loss_optimizer()  \n",
    "        \n",
    "        self.best_val_loss = 10000  # model评测\n",
    "        self.best_val_acc = 0 # model评测2\n",
    "        \n",
    "        self.current_step = -1\n",
    "        \n",
    "        self.training_time = None # 训练时间日期，每次训练时覆盖\n",
    "        \n",
    "        \n",
    "    def _set_loss_optimizer(self):\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        print('loss initialled: cross entropy')\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-5)\n",
    "        print('optimizer initialled: adam')\n",
    "    \n",
    "    def _print_time_now(self):\n",
    "        now = datetime.datetime.today()\n",
    "        print(now)\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()  # 开启训练模式\n",
    "        assert self.model.training\n",
    "        \n",
    "        for batch in self.train:\n",
    "            self.current_step += 1\n",
    "            \n",
    "            q1, q2, y = (list(i) for i in batch)\n",
    "            y_hat = self.model(q1,q2)\n",
    "            y = torch.tensor(y).to(device)\n",
    "             \n",
    "            self.optimizer.zero_grad()\n",
    "            l = self.loss(y_hat, y)\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            \n",
    "            self._post_processing_per_step()\n",
    "        \n",
    "            \n",
    "    def evaluate(self,data_loader, name):\n",
    "        self.model.eval()  # 开启测试模式\n",
    "        assert not self.model.training\n",
    "        self._print_time_now()\n",
    "         \n",
    "        loss = 0\n",
    "        acc_num = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                q1, q2, y = (list(i) for i in batch)\n",
    "                y_hat = self.model.eval()(q1,q2)\n",
    "                y = torch.tensor(y).to(device)\n",
    "        \n",
    "                l = self.loss(y_hat,y)\n",
    "                loss += l\n",
    "                acc_num += torch.sum(y_hat.argmax(dim=1)==y)\n",
    "            \n",
    "            acc = acc_num/(len(data_loader)*data_loader.batch_size)\n",
    "            loss = loss/len(data_loader)\n",
    "            print(f'{name:6s} | loss:{loss:0.4f} | acc:{acc:0.4f}')\n",
    "            \n",
    "            return loss, acc\n",
    "        \n",
    "    def predict_dataloader(self, data_loader):\n",
    "        self.model.eval()  # 开启测试模式\n",
    "        assert not self.model.training\n",
    "        \n",
    "        prediction = []\n",
    "        prediction_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                q1, q2, y = (list(i) for i in batch)\n",
    "                y_hat = self.model.eval()(q1,q2)\n",
    "                y_hat = torch.softmax(y_hat, dim=1) # 归一化logit\n",
    "                y_hat = y_hat.max(dim=1)\n",
    "                pre = y_hat.values\n",
    "                pre_label = y_hat.indices\n",
    "                \n",
    "                prediction += pre.tolist()\n",
    "                prediction_labels += pre_label.tolist()\n",
    "\n",
    "            \n",
    "            return prediction, prediction_labels\n",
    "            \n",
    "              \n",
    "    def trainng(self, epoch=5):\n",
    "        print('start training: ')\n",
    "        self.training_time = datetime.datetime.today()\n",
    "        print(self.training_time)\n",
    "        \n",
    "        self.total_epoch = epoch\n",
    "        torch.set_grad_enabled(True)\n",
    "        assert torch.is_grad_enabled()\n",
    "        \n",
    "        t0 = time()\n",
    "        for e in range(1, epoch+1):\n",
    "            print('-'*42)\n",
    "            self.current_epoch = e\n",
    "            print(f'Epoch: {self.current_epoch}')\n",
    "            \n",
    "            t1 = time()\n",
    "            self._train_one_epoch()\n",
    "            t2 = time()\n",
    "            print(f'cost:{(t2-t1)/3600:0.2f}h')\n",
    "            \n",
    "            self.evaluate(self.train, 'train')\n",
    "            current_val_loss, current_val_acc = self.evaluate(self.val, 'val')\n",
    "            \n",
    "            if self.best_val_loss > current_val_loss:\n",
    "                self.best_val_loss = current_val_loss\n",
    "                torch.save(self.model.state_dict(), './best_val_loss_model')\n",
    "                print('saved best val loss model')\n",
    "                \n",
    "            if self.best_val_acc < current_val_acc:\n",
    "                self.best_val_acc = current_val_acc\n",
    "                torch.save(self.model.state_dict(), './best_val_acc_model')\n",
    "                print('saved best val acc model')\n",
    "            \n",
    "            self._post_processing_per_epoch()\n",
    "                \n",
    "        t3 = time()\n",
    "        print(f'total cost: {(t3-t0)/3600:0.2f}h')\n",
    "        \n",
    "    def _post_processing_per_epoch(self):\n",
    "        pass\n",
    "    \n",
    "    def _post_processing_per_step(self):\n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss initialled: cross entropy\n",
      "optimizer initialled: adam\n",
      "------------------------------------------\n",
      "using the initial model, test.\n",
      "2021-08-21 20:03:40.373932\n",
      "test   | loss:0.6576 | acc:0.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6576, device='cuda:0'), tensor(0.8082, device='cuda:0'))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sBert.load_state_dict(torch.load('init_sbert.model'))\n",
    "trainer = TrainProcessor(sBert, train_loader, val_loader)\n",
    "\n",
    "print('-'*42)\n",
    "print('using the initial model, test.')\n",
    "trainer.evaluate(test_loader, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training: \n",
      "2021-08-21 14:23:14.323652\n",
      "------------------------------------------\n",
      "Epoch: 1\n",
      "cost:0.25h\n",
      "2021-08-21 14:38:21.738531\n",
      "train  | loss:0.1139 | acc:0.9606\n",
      "2021-08-21 14:43:15.096785\n",
      "val    | loss:0.4966 | acc:0.8141\n",
      "saved best val loss model\n",
      "saved best val acc model\n",
      "------------------------------------------\n",
      "Epoch: 2\n",
      "cost:0.26h\n",
      "2021-08-21 14:58:55.287158\n",
      "train  | loss:0.0444 | acc:0.9858\n",
      "2021-08-21 15:03:51.253132\n",
      "val    | loss:0.6135 | acc:0.8230\n",
      "saved best val acc model\n",
      "------------------------------------------\n",
      "Epoch: 3\n",
      "cost:0.26h\n",
      "2021-08-21 15:19:33.101704\n",
      "train  | loss:0.0325 | acc:0.9899\n",
      "2021-08-21 15:24:29.212909\n",
      "val    | loss:0.8297 | acc:0.8083\n",
      "------------------------------------------\n",
      "Epoch: 4\n",
      "cost:0.26h\n",
      "2021-08-21 15:40:10.484704\n",
      "train  | loss:0.0186 | acc:0.9941\n",
      "2021-08-21 15:45:06.698666\n",
      "val    | loss:0.8501 | acc:0.8077\n",
      "------------------------------------------\n",
      "Epoch: 5\n",
      "cost:0.26h\n",
      "2021-08-21 16:00:46.861943\n",
      "train  | loss:0.0148 | acc:0.9952\n",
      "2021-08-21 16:05:42.254819\n",
      "val    | loss:0.9015 | acc:0.8104\n",
      "total cost: 1.71h\n",
      "------------------------------------------\n",
      "load the best LOSS model, then test.\n",
      "2021-08-21 16:05:57.113503\n",
      "test   | loss:0.5106 | acc:0.8059\n",
      "------------------------------------------\n",
      "load the best ACC model, then test.\n",
      "2021-08-21 16:06:11.944150\n",
      "test   | loss:0.6576 | acc:0.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6576, device='cuda:0'), tensor(0.8082, device='cuda:0'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.trainng()\n",
    "\n",
    "\n",
    "print('-'*42)\n",
    "print('load the best LOSS model, then test.')\n",
    "trainer.model.load_state_dict(torch.load('./best_val_loss_model'))\n",
    "trainer.evaluate(test_loader, 'test')\n",
    "\n",
    "\n",
    "print('-'*42)\n",
    "print('load the best ACC model, then test.')\n",
    "trainer.model.load_state_dict(torch.load('./best_val_acc_model'))\n",
    "trainer.evaluate(test_loader, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TrainingProcesserMultistepLR(TrainProcessor):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__(*args)\n",
    "        \n",
    "        \n",
    "    def _set_loss_optimizer(self):\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        print('loss initialled: cross entropy loss')\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-5)\n",
    "        print('optimizer initialled: Adam')\n",
    "        \n",
    "        \n",
    "        def multistep_v1(step):\n",
    "            \n",
    "            batch_size = 32\n",
    "            train_num = 100000\n",
    "            half_steps_one_epoch = (train_num//batch_size)//2\n",
    "            \n",
    "            # 基于经验，半个epoch时第一次decay\n",
    "            # 后续每一个epoch，降一次decay\n",
    "            # 0.5 1.5 2.5 3.5 这样\n",
    "            \n",
    "            \n",
    "            n = (step//half_steps_one_epoch + 1)//2\n",
    "            gamma = 0.1 # decay rate\n",
    "            \n",
    "            return gamma**n\n",
    "        \n",
    "        def multistep_v2(step):\n",
    "            if step >= 1000:\n",
    "                return 0.1\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "        def warmup_multistep(step):\n",
    "            if step<2000:\n",
    "                return step/2000\n",
    "            else:\n",
    "                return 0.1\n",
    "            \n",
    "        def cosine_lr_v1(step):\n",
    "            begin = 1000\n",
    "            total_steps=31250\n",
    "            if step <= begin:\n",
    "                return 1\n",
    "            else:\n",
    "                # angle 从0到pi\n",
    "                angle = np.pi*(step-begin)/(total_steps-begin)\n",
    "                return 0.5*(1 + np.cos(angle))\n",
    "            \n",
    "        def warmup_cosine_v1(step):\n",
    "            begin = 1000\n",
    "            total_steps=31250\n",
    "            if step <= begin:\n",
    "                return step/begin\n",
    "            else:\n",
    "                # angle 从0到pi\n",
    "                angle = np.pi*(step-begin)/(total_steps-begin)\n",
    "                return 0.5*(1 + np.cos(angle))\n",
    "            \n",
    "                \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, warmup_multistep)\n",
    "        \n",
    "        print('scheduler initialled: multistep lr')           \n",
    "    \n",
    "    def _post_processing_per_step(self):\n",
    "        self.scheduler.step() # 更新学习率 \n",
    "        if self.current_step % 1000 == 0:\n",
    "            lr = self.scheduler.get_last_lr()\n",
    "            print(f'steps:{self.current_step} lr = {lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sBert.load_state_dict(torch.load('init_sbert.model'))\n",
    "trainer_multistepLR = TrainingProcesserMultistepLR(sBert, bq_corpus.train_loader, bq_corpus.dev_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainer_multistepLR.trainng()\n",
    "\n",
    "\n",
    "print('-'*42)\n",
    "print('load the best LOSS model, then test.')\n",
    "trainer_multistepLR.model.load_state_dict(torch.load('./best_val_loss_model'))\n",
    "trainer_multistepLR.evaluate(bq_corpus.test_loader, 'test')\n",
    "\n",
    "\n",
    "print('-'*42)\n",
    "print('load the best ACC model, then test.')\n",
    "trainer_multistepLR.model.load_state_dict(torch.load('./best_val_acc_model'))\n",
    "trainer_multistepLR.evaluate(bq_corpus.test_loader, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adversial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainProcessorFgm(TrainProcessor):\n",
    "    \"\"\"\n",
    "    J_hat = alpha*J + alpha*J_ad  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,*args):\n",
    "        super().__init__(*args)\n",
    "        self.emb_name = 'bert.embeddings.word_embeddings.weight'\n",
    "        self.backup = {} # 保存 embedding 参数的值用于恢复\n",
    "        self.grad_backup = {} # 保存 原始grad\n",
    "        \n",
    "        \n",
    "    def _fgm_attack(self,\n",
    "                   epsilon=1.):\n",
    "        \"\"\"\n",
    "        embedding 攻击\n",
    "        \n",
    "        为了代码的简便，不计算过了emb之后的vertor，直接用emb计算。\n",
    "        \"\"\"\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.grad_backup[n] = p.grad  # 保存原始 grad   \n",
    "                \n",
    "                if n == self.emb_name:\n",
    "                    self.backup[n] = p.data.clone()\n",
    "                    norm = torch.norm(p.grad)\n",
    "                    if norm != 0 and not torch.isnan(norm):\n",
    "                        r_at = epsilon*p.grad/norm\n",
    "                        p.data.add_(r_at)\n",
    "        \n",
    "    \n",
    "    def _restore(self):\n",
    "        \"\"\"\n",
    "        更新的时候在原始值基础上更新\n",
    "        \"\"\"\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad and n == self.emb_name:\n",
    "                assert n in self.backup\n",
    "                p.data = self.backup[n]\n",
    "        self.backup = {}\n",
    "        self.grad_backup = {}\n",
    "      \n",
    "    def _cal_grad(self, alpha=0.5):\n",
    "        alpha = 0.7\n",
    "        assert 0<=alpha<=1\n",
    "        \n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                if n == self.emb_name:\n",
    "                    p.grad = self.grad_backup[n]  # emb层的梯度不变\n",
    "                else:\n",
    "                    p.grad = alpha*self.grad_backup[n] + (1-alpha)*p.grad\n",
    "#                     p.grad = self.grad_backup[n] + p.grad\n",
    "        \n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()  # 开启训练模式\n",
    "        assert self.model.training\n",
    "        \n",
    "        for batch in self.train:\n",
    "            q1, q2, y = (list(i) for i in batch)\n",
    "            y_hat = self.model(q1,q2)\n",
    "            y = torch.tensor(y).to(device)\n",
    "             \n",
    "            self.optimizer.zero_grad()\n",
    "            l = self.loss(y_hat, y)\n",
    "            l.backward(retain_graph=True) # 首先正常 back 得到 grad\n",
    "            \n",
    "            self._fgm_attack() # 得到对抗embedding  \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            l_adv = self.loss(y_hat, y)  # 得到攻击后的误差\n",
    "            l_adv.backward() # 求攻击样本的 grad\n",
    "            \n",
    "            self._cal_grad() # 计算 grad 的值\n",
    "            self._restore()  # 恢复embedding 的值, 顺便清空 backup，方便下一轮\n",
    "            self.optimizer.step() # 梯度下降，更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBert.load_state_dict(torch.load('init_sbert.model'))\n",
    "trainer_fgm = TrainProcessorFgm(sBert, train_loader, dev_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_fgm.trainng()\n",
    "print('-'*42)\n",
    "print('load the best LOSS model, then test.')\n",
    "trainer_fgm.model.load_state_dict(torch.load('./best_val_loss_model'))\n",
    "trainer_fgm.evaluate(test_loader, 'test')\n",
    "\n",
    "\n",
    "print('-'*42)\n",
    "print('load the best ACC model, then test.')\n",
    "trainer_fgm.model.load_state_dict(torch.load('./best_val_acc_model'))\n",
    "trainer_fgm.evaluate(test_loader, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fgm+learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainProcessorFgmCosineLr(TrainProcessor):\n",
    "    \"\"\"\n",
    "    J_hat = alpha*J + alpha*J_ad  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,*args):\n",
    "        super().__init__(*args)\n",
    "        self.emb_name = 'bert.embeddings.word_embeddings.weight'\n",
    "        self.backup = {} # 保存 embedding 参数的值用于恢复\n",
    "        self.grad_backup = {} # 保存 原始grad\n",
    "        \n",
    "        \n",
    "    def _fgm_attack(self,\n",
    "                   epsilon=1.):\n",
    "        \"\"\"\n",
    "        embedding 攻击\n",
    "        \n",
    "        为了代码的简便，不计算过了emb之后的vertor，直接用emb计算。\n",
    "        \"\"\"\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.grad_backup[n] = p.grad  # 保存原始 grad   \n",
    "                \n",
    "                if n == self.emb_name:\n",
    "                    self.backup[n] = p.data.clone()\n",
    "                    norm = torch.norm(p.grad)\n",
    "                    if norm != 0 and not torch.isnan(norm):\n",
    "                        r_at = epsilon*p.grad/norm\n",
    "                        p.data.add_(r_at)\n",
    "        \n",
    "    \n",
    "    def _restore(self):\n",
    "        \"\"\"\n",
    "        更新的时候在原始值基础上更新\n",
    "        \"\"\"\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad and n == self.emb_name:\n",
    "                assert n in self.backup\n",
    "                p.data = self.backup[n]\n",
    "        self.backup = {}\n",
    "        self.grad_backup = {}\n",
    "      \n",
    "    def _cal_grad(self, alpha=0.5):\n",
    "        alpha = 0.7\n",
    "        assert 0<=alpha<=1\n",
    "        \n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                if n == self.emb_name:\n",
    "                    p.grad = self.grad_backup[n]  # emb层的梯度不变\n",
    "                else:\n",
    "                    p.grad = alpha*self.grad_backup[n] + (1-alpha)*p.grad\n",
    "#                     p.grad = self.grad_backup[n] + p.grad\n",
    "        \n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()  # 开启训练模式\n",
    "        assert self.model.training\n",
    "        \n",
    "        for batch in self.train:\n",
    "            self.current_step += 1\n",
    "            \n",
    "            q1, q2, y = (list(i) for i in batch)\n",
    "            y_hat = self.model(q1,q2)\n",
    "            y = torch.tensor(y).to(device)\n",
    "             \n",
    "            self.optimizer.zero_grad()\n",
    "            l = self.loss(y_hat, y)\n",
    "            l.backward(retain_graph=True) # 首先正常 back 得到 grad\n",
    "            \n",
    "            self._fgm_attack() # 得到对抗embedding  \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            l_adv = self.loss(y_hat, y)  # 得到攻击后的误差\n",
    "            l_adv.backward() # 求攻击样本的 grad\n",
    "            \n",
    "            self._cal_grad() # 计算 grad 的值\n",
    "            self._restore()  # 恢复embedding 的值, 顺便清空 backup，方便下一轮\n",
    "            self.optimizer.step() # 梯度下降，更新参数\n",
    "            \n",
    "            self._post_processing_per_step()\n",
    "            \n",
    "            \n",
    "    def _set_loss_optimizer(self):\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        print('loss initialled: cross entropy loss')\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-5)\n",
    "        print('optimizer initialled: Adam')\n",
    "        \n",
    "            \n",
    "        def cosine_lr_v1(step):\n",
    "            begin = 1000\n",
    "            total_steps=31250\n",
    "            if step <= begin:\n",
    "                return 1\n",
    "            else:\n",
    "                # angle 从0到pi\n",
    "                angle = np.pi*(step-begin)/(total_steps-begin)\n",
    "                return 0.5*(1 + np.cos(angle))\n",
    "                   \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, cosine_lr_v1)\n",
    "        \n",
    "        print('scheduler initialled: cosine lr')           \n",
    "    \n",
    "    def _post_processing_per_step(self):\n",
    "        self.scheduler.step() # 更新学习率 \n",
    "        if self.current_step % 1000 == 0:\n",
    "            lr = self.scheduler.get_last_lr()\n",
    "            print(f'steps:{self.current_step} lr = {lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainProcessorPGD(TrainProcessor):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__(*args)\n",
    "        self.data_backup = {} # param backup\n",
    "        self.grad_backup={} # \n",
    "        self.emb_name ='bert.embeddings.word_embeddings.weight' \n",
    "        \n",
    "    def attack(self, \n",
    "               epsilon=1., \n",
    "               alpha=0.3,  \n",
    "               t=0):\n",
    "\n",
    "        for n, p in self.model.named_parameters():\n",
    "            # 第一次的时候 backup\n",
    "            if (t==0) and n in self.emb_name: \n",
    "                    self.data_backup[n]=p.data.clone()\n",
    "            if (t==0) and p.requires_grad and p.grad is not None:\n",
    "                self.grad_backup[n]=p.grad.clone()\n",
    "            \n",
    "            # 攻击\n",
    "            if p.requires_grad and self.emb_name in n:\n",
    "                norm=torch.norm(p.grad)\n",
    "                if norm !=0 and not torch.isnan(norm):\n",
    "                    r_at = alpha * p.grad/norm\n",
    "                    p.data.add_(r_at)\n",
    "                    p.data=self.project(n,p.data,epsilon) \n",
    "                \n",
    "    def project(self,param_name, param_data, epsilon):\n",
    "        r = param_data - self.data_backup[param_name]\n",
    "        if torch.norm(r)>epsilon:\n",
    "            r = epsilon*r/torch.norm(r)\n",
    "        return self.data_backup[param_name] + r            \n",
    "    \n",
    "    \n",
    "    def restore(self):\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad and self.emb_name in n: \n",
    "                assert n in self.data_backup\n",
    "                p.data = self.data_backup[n]  # 恢复emb.data\n",
    "                p.grad = self.grad_backup[n]  # 恢复emb.grad \n",
    "        self.data_backup = {}\n",
    "\n",
    "                \n",
    "    def restore_grad(self):\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                p.grad=self.grad_backup[n]\n",
    "\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()  # 开启训练模式\n",
    "        assert self.model.training\n",
    "        \n",
    "        for batch in self.train:\n",
    "            q1, q2, y = (list(i) for i in batch)\n",
    "            y_hat = self.model(q1,q2)\n",
    "            y = torch.tensor(y).to(device)\n",
    "             \n",
    "            self.optimizer.zero_grad()\n",
    "            l = self.loss(y_hat, y)\n",
    "            l.backward(retain_graph=True) # 首先正常back得到grad\n",
    "           \n",
    "            \n",
    "            #PGD对抗训练\n",
    "            K=3\n",
    "            for t in range(K):\n",
    "                self.attack(t=t) # 在embedding上添加对抗扰动, first attack时备份param.data\n",
    "                if t != K-1:\n",
    "                    self.optimizer.zero_grad()\n",
    "                else:\n",
    "                    self.restore_grad()\n",
    "                    loss_adv = self.loss(y_hat, y)\n",
    "                    loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            self.restore()# 恢复embedding参数\n",
    "            #梯度下降，更新参数\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>为什么我无法看到额度</td>\n",
       "      <td>为什么开通了却没有额度</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>为啥换不了</td>\n",
       "      <td>为两次还都提示失败呢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>借了钱，但还没有通过，可以取消吗？</td>\n",
       "      <td>可否取消</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>为什么我申请额度输入密码就一直是那个页面</td>\n",
       "      <td>为什么要输入支付密码来验证</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>今天借 明天还款可以？</td>\n",
       "      <td>今天借明天还要手续费吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     q1             q2  label\n",
       "0            为什么我无法看到额度    为什么开通了却没有额度      0\n",
       "1                 为啥换不了     为两次还都提示失败呢      0\n",
       "2     借了钱，但还没有通过，可以取消吗？           可否取消      1\n",
       "3  为什么我申请额度输入密码就一直是那个页面  为什么要输入支付密码来验证      0\n",
       "4           今天借 明天还款可以？    今天借明天还要手续费吗      0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(test, columns=['q1','q2','label'])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('为什么我无法看到额度', '为啥换不了', '借了钱，但还没有通过，可以取消吗？', '为什么我申请额度输入密码就一直是那个页面', '今天借 明天还款可以？', '你好！今下午咱没有扣我款？', '所借的钱是否可以提现？', '不是邀请的客人就不能借款吗', '人脸失别不了，开不了户', '一天利息好多钱', '为啥还没开放啊', '开通.微粒贷', '咋么才能收到邀请', '扣款时间是几点', '为什么借款总是不通过', '为什么我的无法查看额度', '请问月息多少', '借钱可好取现', '可以开 结清证明吗？', '你好，我银行卡被法院封了，能否换我儿子的卡还款', '一般是什么时候自动扣款？', '我想问什么时候会再打电话过来呢？', '请问有这个手机端的app吗', '开不了户', '不满足微众银行条件', '那我刚刚申请的贷款。。取消掉怎么操作', '什么时候发出邀请呢', '为什么提前还清所有借款不能再借呢？', '我换手机号了', '为什么刚刚借钱要输入很多次验证码和支付密码是不是我手机问题还款就可以', '不借了 不要打来了', '需要提供什么借款材料'), ('为什么开通了却没有额度', '为两次还都提示失败呢', '可否取消', '为什么要输入支付密码来验证', '今天借明天还要手续费吗', '你好  今天怎么没有扣款呢', '该笔借款可以提现吗！', '一般什么样得人会受邀请', '我输入的资料都是正确的，为什么总说不符开户失败？', '1万利息一天是5元是吗', '不是微粒贷客户，怎么可以受邀', '帮我开通', '为什么我6号扣还款的到现在还没', '无利息的还款时间是多久？', '为什么审请不通过', '为什么我点进去没有额度呢', '2万块月息是多少', '可以提现金？', '还清钱后能继续借吗？', '换卡什么时候能换好', '一般几点扣款', '那什么时候打来电话？', '手机信号不好', '人脸失别不了，开不了户', '“您未满足微众银行审批要求，无法查看额度”，这是为什么？什么原因呢', '刚刚申请了贷款，可以取消吗？', '借款没打电话', '提前还款利息怎么算', '如果我换手机怎么办？', '借款连续输验证码和密码', '不想借钱了，但是不小心按到借钱按钮了', '是不是苹果手机 都没开通啊'), tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "for i in test_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre, pre_label = trainer.predict_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['pre_label'] = pre_label\n",
    "df_test['pre'] = pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_label</th>\n",
       "      <th>pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>为什么我无法看到额度</td>\n",
       "      <td>为什么开通了却没有额度</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>为啥换不了</td>\n",
       "      <td>为两次还都提示失败呢</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>借了钱，但还没有通过，可以取消吗？</td>\n",
       "      <td>可否取消</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.934388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>为什么我申请额度输入密码就一直是那个页面</td>\n",
       "      <td>为什么要输入支付密码来验证</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>今天借 明天还款可以？</td>\n",
       "      <td>今天借明天还要手续费吗</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     q1             q2  label  pre_label       pre\n",
       "0            为什么我无法看到额度    为什么开通了却没有额度      0          0  0.999987\n",
       "1                 为啥换不了     为两次还都提示失败呢      0          1  0.557989\n",
       "2     借了钱，但还没有通过，可以取消吗？           可否取消      1          0  0.934388\n",
       "3  为什么我申请额度输入密码就一直是那个页面  为什么要输入支付密码来验证      0          0  0.999917\n",
       "4           今天借 明天还款可以？    今天借明天还要手续费吗      0          0  0.998913"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_representation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1612px",
    "right": "20px",
    "top": "43px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
