from collections import OrderedDict
import torch
from torch import nn

class PetMaskedLanguageModel(nn.Module):
    def __init__(self,
                 name: str,
                 device: str,
                 tokenizer,
                 pretrain_model,
                 max_len: int,
                 **kwargs):
        """
        args：
            name： 模型名字
            device： 'cpu' or 'cuda:0'
            tokenizer: 与pretrain_model 匹配的 tokenizer
            pretrain_model: pretrain_model, 抱抱脸格式
            max_len: tokenize 时句子最大长度
        """
        super().__init__()
        self.name = name
        self.device = device
        self.tokenizer = tokenizer
        self.pretrain_model = pretrain_model
        self.max_len = max_len
        self.final_part = self.__init_layers()

    def __init_layers(self):
        hidden_size = self.pretrain_model.config.hidden_size
        embedding_size = self.pretrain_model.embeddings.word_embeddings.num_embeddings
        final_part = nn.Sequential(OrderedDict([
            ('final_Linear',
             nn.Linear(hidden_size, hidden_size, bias=True)),
            ('final_layernorm',
             nn.LayerNorm(hidden_size, eps=1e-12)),
            ('final_embedding',
             nn.Linear(hidden_size, embedding_size, bias=False))
        ]))
        return final_part

    def forward(self, q1, q2):
        tokens = self.get_tokens(q1, q2)











